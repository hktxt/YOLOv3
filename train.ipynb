{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "from utils.datasets import LoadDataset\n",
    "from utils import torch_utils\n",
    "import time\n",
    "import rtest\n",
    "from tqdm import tqdm\n",
    "from models import *\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CUDA device0 _CudaDeviceProperties(name='GeForce RTX 2070', total_memory=8192MB)\n"
     ]
    }
   ],
   "source": [
    "# specify visible GPU\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'\n",
    "device = torch_utils.select_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = 416\n",
    "EPOCHES = 270\n",
    "BATCH_SIZE = 10\n",
    "START_EPOCH = 0\n",
    "CFG = 'cfg/yolov3-tiny.cfg'\n",
    "DATA_CFG = 'cfg/coco-h.data'\n",
    "NUM_WORKERS = 5\n",
    "FREEZE_BACKBONE = False\n",
    "FROM_SCRATCH = True\n",
    "\n",
    "weights = 'F:/Dev/weights/'\n",
    "latest = os.path.join(weights, 'latest.pt')\n",
    "best = os.path.join(weights, 'best.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:/coco/trainvalno5kn.txt'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_path = parse_data_cfg(DATA_CFG)['train'];train_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 416, 416])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dataset\n",
    "dataset = LoadDataset(train_path, img_size=IMG_SIZE);dataset[1][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "model = Darknet(CFG, IMG_SIZE).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataloader\n",
    "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS, shuffle=False, pin_memory=True, collate_fn=dataset.collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check Dataloader\n",
    "for i, (imgs, targets, _, _) in enumerate(dataloader):\n",
    "    #print(targets.shape)\n",
    "    plot_images(imgs=imgs, targets=targets, fname='train_batch0.jpg')\n",
    "    if i == 0:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "cutoff = -1  # backbone reaches to cutoff layer\n",
    "START_EPOCH = 0\n",
    "best_loss = float('inf')\n",
    "\n",
    "if not FROM_SCRATCH:\n",
    "    if '-tiny.cfg' in CFG:\n",
    "        cutoff = load_darknet_weights(model, weights + 'yolov3-tiny.conv.15')\n",
    "    else:\n",
    "        cutoff = load_darknet_weights(model, weights + 'darknet53.conv.74')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.device_count() > 1:\n",
    "    print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
    "    model = nn.DataParallel(model)\n",
    "model.to(device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "layer                                     name  gradient   parameters                shape         mu      sigma\n",
      "    0                          0.conv_0.weight      True          432        [16, 3, 3, 3]   -0.00889      0.109\n",
      "    1                    0.batch_norm_0.weight      True           16                 [16]      0.539      0.365\n",
      "    2                      0.batch_norm_0.bias      True           16                 [16]          0          0\n",
      "    3                          2.conv_2.weight      True         4608       [32, 16, 3, 3]  -0.000259     0.0479\n",
      "    4                    2.batch_norm_2.weight      True           32                 [32]       0.49      0.276\n",
      "    5                      2.batch_norm_2.bias      True           32                 [32]          0          0\n",
      "    6                          4.conv_4.weight      True        18432       [64, 32, 3, 3]   -0.00017     0.0341\n",
      "    7                    4.batch_norm_4.weight      True           64                 [64]      0.463      0.308\n",
      "    8                      4.batch_norm_4.bias      True           64                 [64]          0          0\n",
      "    9                          6.conv_6.weight      True        73728      [128, 64, 3, 3]   0.000187      0.024\n",
      "   10                    6.batch_norm_6.weight      True          128                [128]        0.5      0.285\n",
      "   11                      6.batch_norm_6.bias      True          128                [128]          0          0\n",
      "   12                          8.conv_8.weight      True       294912     [256, 128, 3, 3]   1.54e-05      0.017\n",
      "   13                    8.batch_norm_8.weight      True          256                [256]      0.495      0.311\n",
      "   14                      8.batch_norm_8.bias      True          256                [256]          0          0\n",
      "   15                        10.conv_10.weight      True  1.17965e+06     [512, 256, 3, 3]  -2.03e-06      0.012\n",
      "   16                  10.batch_norm_10.weight      True          512                [512]      0.513      0.292\n",
      "   17                    10.batch_norm_10.bias      True          512                [512]          0          0\n",
      "   18                        12.conv_12.weight      True  4.71859e+06    [1024, 512, 3, 3]   2.83e-06    0.00851\n",
      "   19                  12.batch_norm_12.weight      True         1024               [1024]      0.492       0.29\n",
      "   20                    12.batch_norm_12.bias      True         1024               [1024]          0          0\n",
      "   21                        13.conv_13.weight      True       262144    [256, 1024, 1, 1]   6.45e-05      0.018\n",
      "   22                  13.batch_norm_13.weight      True          256                [256]      0.503      0.297\n",
      "   23                    13.batch_norm_13.bias      True          256                [256]          0          0\n",
      "   24                        14.conv_14.weight      True  1.17965e+06     [512, 256, 3, 3]   -2.4e-05      0.012\n",
      "   25                  14.batch_norm_14.weight      True          512                [512]      0.503      0.287\n",
      "   26                    14.batch_norm_14.bias      True          512                [512]          0          0\n",
      "   27                        15.conv_15.weight      True       130560     [255, 512, 1, 1]  -0.000102     0.0255\n",
      "   28                          15.conv_15.bias      True          255                [255]   -0.00135     0.0254\n",
      "   29                        18.conv_18.weight      True        32768     [128, 256, 1, 1]  -0.000176     0.0363\n",
      "   30                  18.batch_norm_18.weight      True          128                [128]      0.515      0.299\n",
      "   31                    18.batch_norm_18.bias      True          128                [128]          0          0\n",
      "   32                        21.conv_21.weight      True       884736     [256, 384, 3, 3]  -4.58e-07    0.00982\n",
      "   33                  21.batch_norm_21.weight      True          256                [256]      0.531      0.275\n",
      "   34                    21.batch_norm_21.bias      True          256                [256]          0          0\n",
      "   35                        22.conv_22.weight      True        65280     [255, 256, 1, 1]  -0.000211      0.036\n",
      "   36                          22.conv_22.bias      True          255                [255]    0.00114      0.035\n",
      "Model Summary: 37 layers, 8.85237e+06 parameters, 8.85237e+06 gradients\n"
     ]
    }
   ],
   "source": [
    "model_info(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer\n",
    "lr0 = 0.001  # initial learning rate\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr0, momentum=0.9, weight_decay=0.0005)\n",
    "\n",
    "# Scheduler (reduce lr at epochs 218, 245, i.e. batches 400k, 450k)\n",
    "scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[218, 245], gamma=0.1,\n",
    "                                                 last_epoch=START_EPOCH - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   0/269 11618/11726      1.15     0.378      5.54      5.33      12.4        84    0.0658\n",
      "   0/269 11619/11726      1.15     0.378      5.54      5.33      12.4       132    0.0658\n",
      "   0/269 11620/11726      1.15     0.378      5.54      5.33      12.4        89    0.0648\n",
      "   0/269 11621/11726      1.15     0.378      5.54      5.33      12.4        64    0.0608\n",
      "   0/269 11622/11726      1.15     0.378      5.54      5.33      12.4        78    0.0608\n",
      "   0/269 11623/11726      1.15     0.378      5.54      5.33      12.4        81    0.0618\n",
      "   0/269 11624/11726      1.15     0.378      5.54      5.33      12.4        58    0.0648\n",
      "   0/269 11625/11726      1.15     0.378      5.54      5.33      12.4        48    0.0618\n",
      "   0/269 11626/11726      1.15     0.378      5.54      5.33      12.4        68    0.0668\n",
      "   0/269 11627/11726      1.15     0.378      5.54      5.33      12.4        89    0.0598\n",
      "   0/269 11628/11726      1.15     0.378      5.54      5.33      12.4        55    0.0608\n",
      "   0/269 11629/11726      1.15     0.378      5.54      5.33      12.4       114    0.0638\n",
      "   0/269 11630/11726      1.15     0.378      5.54      5.33      12.4        59    0.0648\n",
      "   0/269 11631/11726      1.15     0.378      5.54      5.33      12.4        92    0.0608\n",
      "   0/269 11632/11726      1.15     0.378      5.54      5.33      12.4        87    0.0588\n",
      "   0/269 11633/11726      1.15     0.378      5.54      5.33      12.4        33    0.0598\n",
      "   0/269 11634/11726      1.15     0.378      5.54      5.33      12.4        70    0.0568\n",
      "   0/269 11635/11726      1.15     0.378      5.54      5.33      12.4        90    0.0758\n",
      "   0/269 11636/11726      1.15     0.378      5.54      5.33      12.4        79    0.0628\n",
      "   0/269 11637/11726      1.15     0.378      5.54      5.33      12.4       129    0.0628\n",
      "   0/269 11638/11726      1.15     0.378      5.54      5.33      12.4        85    0.0598\n",
      "   0/269 11639/11726      1.15     0.378      5.54      5.33      12.4       114    0.0588\n",
      "   0/269 11640/11726      1.15     0.378      5.53      5.33      12.4        84    0.0618\n",
      "   0/269 11641/11726      1.15     0.378      5.53      5.33      12.4       101    0.0628\n",
      "   0/269 11642/11726      1.15     0.378      5.53      5.33      12.4        74    0.0618\n",
      "   0/269 11643/11726      1.15     0.378      5.53      5.33      12.4        43    0.0568\n",
      "   0/269 11644/11726      1.15     0.378      5.53      5.33      12.4       100    0.0588\n",
      "   0/269 11645/11726      1.15     0.378      5.53      5.33      12.4        78    0.0598\n",
      "   0/269 11646/11726      1.15     0.378      5.53      5.33      12.4        50    0.0588\n",
      "   0/269 11647/11726      1.15     0.378      5.53      5.33      12.4        67    0.0578\n",
      "   0/269 11648/11726      1.15     0.378      5.53      5.33      12.4        74    0.0598\n",
      "   0/269 11649/11726      1.15     0.378      5.53      5.33      12.4        50    0.0628\n",
      "   0/269 11650/11726      1.15     0.378      5.53      5.33      12.4        83    0.0618\n",
      "   0/269 11651/11726      1.15     0.378      5.53      5.33      12.4        52    0.0608\n",
      "   0/269 11652/11726      1.15     0.378      5.53      5.33      12.4        68    0.0658\n",
      "   0/269 11653/11726      1.15     0.378      5.53      5.33      12.4        61    0.0578\n",
      "   0/269 11654/11726      1.15     0.378      5.53      5.33      12.4        78    0.0628\n",
      "   0/269 11655/11726      1.15     0.378      5.53      5.33      12.4        70    0.0598\n",
      "   0/269 11656/11726      1.15     0.378      5.53      5.33      12.4        71    0.0618\n",
      "   0/269 11657/11726      1.15     0.378      5.53      5.33      12.4        78    0.0588\n",
      "   0/269 11658/11726      1.15     0.378      5.53      5.33      12.4        46    0.0618\n",
      "   0/269 11659/11726      1.15     0.377      5.53      5.33      12.4        55    0.0668\n",
      "   0/269 11660/11726      1.15     0.377      5.53      5.33      12.4       104    0.0768\n",
      "   0/269 11661/11726      1.15     0.377      5.53      5.33      12.4        83    0.0618\n",
      "   0/269 11662/11726      1.15     0.377      5.53      5.33      12.4        73    0.0628\n",
      "   0/269 11663/11726      1.15     0.377      5.53      5.33      12.4       119    0.0638\n",
      "   0/269 11664/11726      1.15     0.377      5.53      5.33      12.4        54    0.0618\n",
      "   0/269 11665/11726      1.15     0.377      5.53      5.33      12.4        95    0.0658\n",
      "   0/269 11666/11726      1.15     0.377      5.53      5.33      12.4        86    0.0658\n",
      "   0/269 11667/11726      1.15     0.377      5.53      5.33      12.4        58    0.0618\n",
      "   0/269 11668/11726      1.15     0.377      5.53      5.33      12.4        74    0.0628\n",
      "   0/269 11669/11726      1.15     0.377      5.53      5.33      12.4        53    0.0588\n",
      "   0/269 11670/11726      1.15     0.377      5.53      5.33      12.4        87    0.0608\n",
      "   0/269 11671/11726      1.15     0.377      5.53      5.33      12.4        60    0.0698\n",
      "   0/269 11672/11726      1.15     0.377      5.53      5.33      12.4        97    0.0768\n",
      "   0/269 11673/11726      1.15     0.377      5.53      5.33      12.4        71    0.0568\n",
      "   0/269 11674/11726      1.15     0.377      5.53      5.33      12.4        80    0.0598\n",
      "   0/269 11675/11726      1.15     0.377      5.53      5.33      12.4        79    0.0608\n",
      "   0/269 11676/11726      1.15     0.377      5.53      5.33      12.4        49    0.0698\n",
      "   0/269 11677/11726      1.15     0.377      5.53      5.33      12.4        43    0.0608\n",
      "   0/269 11678/11726      1.15     0.377      5.53      5.33      12.4        75    0.0598\n",
      "   0/269 11679/11726      1.15     0.377      5.53      5.33      12.4        87    0.0608\n",
      "   0/269 11680/11726      1.15     0.377      5.53      5.33      12.4       105    0.0578\n",
      "   0/269 11681/11726      1.15     0.377      5.53      5.33      12.4        67    0.0608\n",
      "   0/269 11682/11726      1.15     0.377      5.53      5.33      12.4        66    0.0668\n",
      "   0/269 11683/11726      1.15     0.377      5.53      5.33      12.4        73    0.0668\n",
      "   0/269 11684/11726      1.15     0.377      5.53      5.33      12.4        54    0.0598\n",
      "   0/269 11685/11726      1.15     0.377      5.52      5.33      12.4        58    0.0628\n",
      "   0/269 11686/11726      1.15     0.377      5.52      5.33      12.4        84    0.0588\n",
      "   0/269 11687/11726      1.15     0.377      5.52      5.33      12.4        55    0.0618\n",
      "   0/269 11688/11726      1.15     0.377      5.52      5.33      12.4        43    0.0598\n",
      "   0/269 11689/11726      1.15     0.377      5.52      5.33      12.4        75    0.0698\n",
      "   0/269 11690/11726      1.15     0.377      5.52      5.33      12.4       106    0.0689\n",
      "   0/269 11691/11726      1.15     0.377      5.52      5.33      12.4        55    0.0599\n",
      "   0/269 11692/11726      1.15     0.377      5.52      5.33      12.4        98    0.0608\n",
      "   0/269 11693/11726      1.15     0.377      5.52      5.33      12.4        44    0.0678\n",
      "   0/269 11694/11726      1.15     0.377      5.52      5.33      12.4        65    0.0579\n",
      "   0/269 11695/11726      1.15     0.377      5.52      5.33      12.4        86    0.0649\n",
      "   0/269 11696/11726      1.15     0.377      5.52      5.33      12.4        52    0.0638\n",
      "   0/269 11697/11726      1.15     0.377      5.52      5.33      12.4        86    0.0618\n",
      "   0/269 11698/11726      1.15     0.377      5.52      5.33      12.4        54    0.0588\n",
      "   0/269 11699/11726      1.15     0.377      5.52      5.33      12.4        51    0.0608\n",
      "   0/269 11700/11726      1.15     0.377      5.52      5.33      12.4        49    0.0628\n",
      "   0/269 11701/11726      1.15     0.377      5.52      5.33      12.4        63    0.0588\n",
      "   0/269 11702/11726      1.15     0.377      5.52      5.33      12.4        64    0.0688\n",
      "   0/269 11703/11726      1.15     0.377      5.52      5.32      12.4        64    0.0618\n",
      "   0/269 11704/11726      1.15     0.377      5.52      5.32      12.4        25    0.0758\n",
      "   0/269 11705/11726      1.15     0.377      5.52      5.32      12.4        96    0.0598\n",
      "   0/269 11706/11726      1.15     0.377      5.52      5.32      12.4        49    0.0648\n",
      "   0/269 11707/11726      1.15     0.377      5.52      5.32      12.4        43    0.0768\n",
      "   0/269 11708/11726      1.15     0.377      5.52      5.32      12.4        55    0.0608\n",
      "   0/269 11709/11726      1.15     0.377      5.52      5.32      12.4        71    0.0638\n",
      "   0/269 11710/11726      1.15     0.377      5.52      5.32      12.4        90    0.0668\n",
      "   0/269 11711/11726      1.15     0.377      5.52      5.32      12.4        64    0.0638\n",
      "   0/269 11712/11726      1.15     0.377      5.52      5.32      12.4        52    0.0578\n",
      "   0/269 11713/11726      1.15     0.377      5.52      5.32      12.4        76    0.0608\n",
      "   0/269 11714/11726      1.15     0.377      5.52      5.32      12.4       106    0.0598\n",
      "   0/269 11715/11726      1.15     0.377      5.52      5.32      12.4        71    0.0608\n",
      "   0/269 11716/11726      1.15     0.377      5.52      5.32      12.4        92    0.0618\n",
      "   0/269 11717/11726      1.15     0.377      5.52      5.32      12.4        82    0.0598\n",
      "   0/269 11718/11726      1.15     0.377      5.52      5.32      12.4       152    0.0618\n",
      "   0/269 11719/11726      1.15     0.377      5.52      5.32      12.4        65    0.0579\n",
      "   0/269 11720/11726      1.15     0.377      5.52      5.32      12.4        49    0.0599\n",
      "   0/269 11721/11726      1.15     0.377      5.52      5.32      12.4        72    0.0588\n",
      "   0/269 11722/11726      1.15     0.377      5.52      5.32      12.4        63    0.0588\n",
      "   0/269 11723/11726      1.15     0.377      5.52      5.32      12.4       121    0.0608\n",
      "   0/269 11724/11726      1.15     0.377      5.52      5.32      12.4       108    0.0608\n",
      "   0/269 11725/11726      1.15     0.377      5.52      5.32      12.4        73    0.0588\n",
      "   0/269 11726/11726      1.15     0.377      5.52      5.32      12.4        13    0.0509\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing mAP:  10%|â–ˆ         | 50/500 [00:45<04:52,  1.54it/s]"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "Traceback (most recent call last):\n  File \"E:\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 138, in _worker_loop\n    samples = collate_fn([dataset[i] for i in batch_indices])\n  File \"E:\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 138, in <listcomp>\n    samples = collate_fn([dataset[i] for i in batch_indices])\n  File \"F:\\github\\YOLOv3\\utils\\datasets.py\", line 35, in __getitem__\n    with open(label_pth, 'r') as f:\nFileNotFoundError: [Errno 2] No such file or directory: 'C:/coco/labels/val2014/COCO_val2014_000000058636.txt'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-b77ff2ecbf16>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;31m# Calculate mAP\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m         \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrtest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mCFG\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDATA_CFG\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimg_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mIMG_SIZE\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconf_thres\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m     \u001b[1;31m# Write epoch results\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\github\\YOLOv3\\rtest.py\u001b[0m in \u001b[0;36mtest\u001b[1;34m(cfg, data_cfg, weights, batch_size, img_size, iou_thres, conf_thres, nms_thres, model)\u001b[0m\n\u001b[0;32m     45\u001b[0m     \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmap\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmf1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m     \u001b[0mjdict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstats\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0map\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0map_class\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 47\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mbatch_i\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mimgs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpaths\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshapes\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtqdm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'Computing mAP'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     48\u001b[0m         \u001b[0mtargets\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtargets\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m         \u001b[0mimgs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimgs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\tqdm\\_tqdm.py\u001b[0m in \u001b[0;36m__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1020\u001b[0m                 \"\"\"), fp_write=getattr(self.fp, 'write', sys.stderr.write))\n\u001b[0;32m   1021\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1022\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1023\u001b[0m                 \u001b[1;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1024\u001b[0m                 \u001b[1;31m# Update and possibly print the progressbar.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    635\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreorder_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    636\u001b[0m                 \u001b[1;32mcontinue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 637\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_process_next_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    638\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    639\u001b[0m     \u001b[0mnext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m__next__\u001b[0m  \u001b[1;31m# Python 2 compatibility\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m_process_next_batch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    656\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_put_indices\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    657\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 658\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexc_msg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    659\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    660\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: Traceback (most recent call last):\n  File \"E:\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 138, in _worker_loop\n    samples = collate_fn([dataset[i] for i in batch_indices])\n  File \"E:\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 138, in <listcomp>\n    samples = collate_fn([dataset[i] for i in batch_indices])\n  File \"F:\\github\\YOLOv3\\utils\\datasets.py\", line 35, in __getitem__\n    with open(label_pth, 'r') as f:\nFileNotFoundError: [Errno 2] No such file or directory: 'C:/coco/labels/val2014/COCO_val2014_000000058636.txt'\n"
     ]
    }
   ],
   "source": [
    "# Start training\n",
    "t = time.time()\n",
    "nB = len(dataloader) # num of batches\n",
    "n_burnin = min(round(nB / 5 + 1), 1000)\n",
    "accumulate = 1\n",
    "multi_scale = False\n",
    "\n",
    "for epoch in range(START_EPOCH, EPOCHES):\n",
    "    model.train()\n",
    "    print(('\\n%8s%12s' + '%10s' * 7) % ('Epoch', 'Batch', 'xy', 'wh', 'conf', 'cls', 'total', 'nTargets', 'time'))\n",
    "\n",
    "    # Update scheduler\n",
    "    scheduler.step()\n",
    "\n",
    "    # Freeze backbone at epoch 0, unfreeze at epoch 1\n",
    "    if FREEZE_BACKBONE and epoch < 2:\n",
    "        for name, p in model.named_parameters():\n",
    "            if int(name.split('.')[1]) < cutoff:  # if layer < 75\n",
    "                p.requires_grad = False if epoch == 0 else True\n",
    "\n",
    "    mloss = torch.zeros(5).to(device) # mean losses\n",
    "    for i, (imgs, targets, _, _) in enumerate(dataloader):\n",
    "        imgs = imgs.to(device)\n",
    "        targets = targets.to(device)\n",
    "        nt = len(targets)\n",
    "\n",
    "        # SGD burn-in\n",
    "        if epoch == 0 and i <= n_burnin:\n",
    "            lr = lr0 * (i / n_burnin) ** 4\n",
    "            for x in optimizer.param_groups:\n",
    "                x['lr'] = lr\n",
    "\n",
    "        # Run model\n",
    "        pred = model(imgs)\n",
    "\n",
    "        # Build targets\n",
    "        target_list = build_targets(model, targets)\n",
    "\n",
    "        # Compute loss\n",
    "        loss, loss_items = compute_loss(pred, target_list)\n",
    "\n",
    "        # Compute gradient\n",
    "        loss.backward()\n",
    "\n",
    "        # Accumulate gradient for x batches before optimizing\n",
    "        if (i + 1) % accumulate == 0 or (i + 1) == nB:\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        # Update running mean of tracked metrics\n",
    "        mloss = (mloss * i + loss_items) / (i + 1)\n",
    "\n",
    "        # Print batch results\n",
    "        s = ('%8s%12s' + '%10.3g' * 7) % (\n",
    "            '%g/%g' % (epoch, EPOCHES - 1),\n",
    "            '%g/%g' % (i, nB - 1), *mloss, nt, time.time() - t)\n",
    "        t = time.time()\n",
    "        print(s)\n",
    "\n",
    "        # Multi-Scale training (320 - 608 pixels) every 10 batches\n",
    "        if multi_scale and (i + 1) % 10 == 0:\n",
    "            dataset.img_size = random.choice(range(10, 20)) * 32\n",
    "            print('multi_scale img_size = %g' % dataset.img_size)\n",
    "\n",
    "    # Calculate mAP\n",
    "    with torch.no_grad():\n",
    "        results = rtest.test(CFG, DATA_CFG, batch_size=BATCH_SIZE, img_size=IMG_SIZE, model=model, conf_thres=0.1)\n",
    "\n",
    "    # Write epoch results\n",
    "    with open('results.txt', 'a') as file:\n",
    "        file.write(s + '%11.3g' * 5 % results + '\\n')  # P, R, mAP, F1, test_loss\n",
    "\n",
    "    # Update best loss\n",
    "    test_loss = results[4]\n",
    "    if test_loss < best_loss:\n",
    "        best_loss = test_loss\n",
    "\n",
    "    # Save training results\n",
    "    save = True\n",
    "    if save:\n",
    "        # Create checkpoint\n",
    "        chkpt = {'epoch': epoch,\n",
    "                 'best_loss': best_loss,\n",
    "                 'model': model.module.state_dict() if type(\n",
    "                     model) is nn.parallel.DistributedDataParallel else model.state_dict(),\n",
    "                 'optimizer': optimizer.state_dict()}\n",
    "\n",
    "        # Save latest checkpoint\n",
    "        torch.save(chkpt, latest)\n",
    "\n",
    "        # Save best checkpoint\n",
    "        if best_loss == test_loss:\n",
    "            torch.save(chkpt, best)\n",
    "\n",
    "        # Save backup every 10 epochs (optional)\n",
    "        if epoch > 0 and epoch % 10 == 0:\n",
    "            torch.save(chkpt, weights + 'backup%g.pt' % epoch)\n",
    "\n",
    "        # Delete checkpoint\n",
    "        del chkpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
